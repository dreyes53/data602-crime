# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q363vrE9hdmvNRgEPdYOZDn0DWdlOBhP
"""

# !pip install geopandas descartes folium mapclassify prophet

# Download Map data for Prince George's (PG) County
import os
import os.path
import requests
import zipfile
import pandas as pd
import json
import matplotlib.pyplot as plt
import descartes
import geopandas as gpd
from shapely.geometry import Point, Polygon
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from xgboost import XGBClassifier
import plotly.express as px
from prophet import Prophet
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import sqlite3

# Get Historical Crime Incidents February 2017 to 5th July 2023 for PG county
conn = sqlite3.connect('historical_crime_data.db')
historical_df = pd.read_sql('SELECT * FROM crimes', conn)

# Get Crime Incidents July 2023 to Present and merge with Historical crime data
# Upcoming Weekly Schedule
# November 24, 2023
# Decemeber 1, 2023
# December 8, 2023
# December 15, 2023

offset = 0
limit = 10000
full_present_crime_data = []

while True:
  url = f'https://data.princegeorgescountymd.gov/resource/xjru-idbe.json?$offset={offset}&$limit={limit}'
  x = requests.get(url)
  nested = json.loads(json.dumps(x.json(), indent=2))
  if len(nested) == 0:
    break
  full_present_crime_data = full_present_crime_data + nested
  offset = offset + limit

print("Number of Crime Incidents July 2023 to Present:", len(full_present_crime_data))
present_df = pd.json_normalize(full_present_crime_data)
present_df = present_df[present_df['location'].notna()]
present_df = present_df.drop(columns=['location'])

frames = [historical_df, present_df]
historical_df = pd.concat(frames, ignore_index=True)


historical_df['date']= pd.to_datetime(historical_df['date'])
historical_df['day_of_week'] = historical_df['date'].dt.dayofweek
historical_df['day_of_week'] = historical_df['day_of_week'].map({
    0: 'Monday',
    1: 'Tuesday',
    2: 'Wednesday',
    3: 'Thursday',
    4: 'Friday',
    5: 'Saturday',
    6: 'Sunday'
})
historical_df['latitude'] = pd.to_numeric(historical_df['latitude'])
historical_df['longitude'] = pd.to_numeric(historical_df['longitude'])

historical_df = historical_df[(historical_df['latitude'] > 30) & (historical_df['latitude'] < 40)]
historical_df = historical_df[(historical_df['longitude'] > -80) & (historical_df['longitude'] < -75)]

print("Number of Crime Incidents February 2017 to Present:", len(historical_df.index))

# Map data gotten from here: https://catalog.data.gov/dataset/tiger-line-shapefile-2018-county-prince-georges-county-md-all-roads-county-based-shapefile
street_map = gpd.read_file('./map_data/tl_2018_24033_roads.shp')

# Create Geodataframe
crs = {'init':'epsg:4326'}
geometry = [Point(xy) for xy in zip(historical_df['longitude'], historical_df['latitude'])]

print('Generating PG County Geopandas dataframe..')
geo_df = gpd.GeoDataFrame(data=historical_df, #specify our data
                          crs=crs, #specify our coordinate reference system
                          geometry=geometry) #specify the geometry list we created

# # Plot Criminal Activities by Lat and Long
fig, ax = plt.subplots(figsize=(15,15))
street_map.plot(ax=ax, alpha=0.4, color='grey')

incident_grouped = geo_df.groupby('clearance_code_inc_type')

for group_name, df_group in incident_grouped:
  geo_df[geo_df['clearance_code_inc_type'] == group_name].plot(ax=ax,
                                       markersize=20,
                                       marker='o',
                                       label=group_name.title())
# plt.legend(prop={'size':15})
ax.legend(loc='center right', bbox_to_anchor=(1.5, 0.5))
ax.set_title('PG County Crime Map')
fig.savefig('./plot_images/pg_county_map.jpg', bbox_inches='tight')

# Print the value counts of the categories
fig, ax = plt.subplots(figsize=(15,15))
sns.countplot(x = historical_df['clearance_code_inc_type'], orient='v', order = historical_df['clearance_code_inc_type'].value_counts().index, ax=ax)
ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)
ax.set_title('PG County Crime Categories')
fig.savefig('./plot_images/crime_categories.jpg', bbox_inches='tight')

# Print Crime Counts per Weekday
fig, ax = plt.subplots(figsize=(15,15))
sns.countplot(y = historical_df['day_of_week'], orient='h', order = historical_df['day_of_week'].value_counts().index, ax=ax)
ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)
ax.set_title('PG County Crime by Days of the Week')
fig.savefig('./plot_images/crime_day_of_week.jpg', bbox_inches='tight')


# Processing Function for Features
def cart2polar(x, y):
    dist = np.sqrt(x**2 + y**2)
    phi = np.arctan2(y, x)
    return dist, phi

def preprocessFeatures(dfx):

    # Time Feature Engineering
    df = pd.get_dummies(dfx[['day_of_week' , 'pgpd_sector']])
    # df['Hour_Min'] = pd.to_datetime(dfx['Dates']).dt.hour + pd.to_datetime(dfx['Dates']).dt.minute / 60
    # # We add a feature that contains the expontential time
    # df['Hour_Min_Exp'] = np.exp(df['Hour_Min'])

    df['Day'] = pd.to_datetime(dfx['date']).dt.day
    df['Month'] = pd.to_datetime(dfx['date']).dt.month
    df['Year'] = pd.to_datetime(dfx['date']).dt.year

    month_one_hot_encoded = pd.get_dummies(pd.to_datetime(dfx['date']).dt.month, prefix='Month')
    df = pd.concat([df, month_one_hot_encoded], axis=1, join="inner")

    # Convert Carthesian Coordinates to Polar Coordinates
    df[['longitude', 'latitude']] = dfx[['longitude', 'latitude']] # we maintain the original coordindates as additional features
    df['dist'], df['phi'] = cart2polar(dfx['longitude'], dfx['latitude'])

    # Extracting Street Types
    df['Is_ST'] = dfx['street_address'].str.contains(" ST", case=True)
    df['Is_AV'] = dfx['street_address'].str.contains(" AV", case=True)
    df['Is_WY'] = dfx['street_address'].str.contains(" WY", case=True)
    df['Is_TR'] = dfx['street_address'].str.contains(" TR", case=True)
    df['Is_DR'] = dfx['street_address'].str.contains(" DR", case=True)
    df['Is_Block'] = dfx['street_address'].str.contains(" Block", case=True)
    df['Is_crossing'] = dfx['street_address'].str.contains(" / ", case=True)

    return df

# Processing Function for Labels
def encodeLabels(dfx):
    df = pd.DataFrame (columns = [])
    factor = pd.factorize(dfx['clearance_code_inc_type'])
    return factor

# Remove Outliers by Longitude
# df_cleaned = historical_df[historical_df['latitude']<70]
df_cleaned = historical_df

# Encode Labels as Integer
factor = encodeLabels(df_cleaned)
y_df = factor[0]
labels = list(factor[1])

# Create train_df & test_df
x_df = preprocessFeatures(df_cleaned).copy()

# Split the data into x_train and y_train data sets
x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, train_size=0.7, random_state=0)
x_train

# Train a single random forest classifier - parameters are a best guess
clf = RandomForestClassifier(max_depth=100, random_state=0, n_estimators = 200)
clf.fit(x_train, y_train.ravel())
y_pred = clf.predict(x_test)

results_log = classification_report(y_test, y_pred)
print(results_log)

# Configure the XGBoost model
param = {'booster': 'gbtree',
         'tree_method': 'gpu_hist',
         'predictor': 'gpu_predictor',
         'max_depth': 140,
         'eta': 0.3,
         'objective': '{multi:softmax}',
         'eval_metric': 'mlogloss',
         'num_round': 30,
         'feature_selector ': 'cyclic'
        }

xgb_clf = XGBClassifier(param)
xgb_clf.fit(x_train, y_train.ravel())
score = xgb_clf.score(x_test, y_test.ravel())
print(score)

# Create predictions on the test dataset
y_pred = xgb_clf.predict(x_test)

# Print a classification report
results_log = classification_report(y_test, y_pred)
print(results_log)

# Print a multi-Class Confusion Matrix
cnf_matrix = confusion_matrix(y_test.reshape(-1), y_pred)
df_cm = pd.DataFrame(cnf_matrix, columns=np.unique(y_test), index = np.unique(y_test))
df_cm.index.name = 'Actual'
df_cm.columns.name = 'Predicted'
plt.figure(figsize = (16,12))
plt.tight_layout()
sns.set(font_scale=1.4) #for label size
sns.heatmap(df_cm, cbar=True, cmap= "inferno", annot=False, fmt='.0f' #, annot_kws={"size": 13}
           )
plt.title("PG County Confusion Matrix")
plt.savefig('./plot_images/crime_confusion_matrix.jpg', bbox_inches='tight')
exit()